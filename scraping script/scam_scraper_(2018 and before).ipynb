{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5f4a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6372fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractors = {'username': re.compile('username:([^\\n]+)'),\n",
    "              'name': re.compile('\\nname:([^\\n]+)'),\n",
    "              'age': re.compile('\\nage:([^\\n]+)'),\n",
    "              'location': re.compile('\\nlocation :([^\\n]+)'),\n",
    "              'ethnicity': re.compile('\\nethnicity :([^\\n]+)'),\n",
    "              'occupation': re.compile('\\noccupation:([^\\n]+)'),\n",
    "              'status': re.compile('\\nmarital status:([^\\n]+)'),\n",
    "              'phone': re.compile('\\ntel:([^\\n]+)'),\n",
    "              'inet': re.compile('\\nIP address:([^\\n]+)'),\n",
    "              'email': re.compile('\\nemail:([^\\n]+)'),\n",
    "              'description': re.compile('\\ndescription: ([\\n\\w\\W]+)\\nmessage'),\n",
    "              'messages': re.compile('\\nmessage: ([\\n\\w\\W]+)\\nWHY IS'),\n",
    "              'justifications': re.compile('\\nWHY IS IT A SCAM / FAKE([\\n\\w\\W]+)\\W This post')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a9414b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-01 : Begin indexing.\n",
      "2014-01 : 29 profiles\n",
      "2014-01 : complete.\n",
      "2014-02 : Begin indexing.\n",
      "2014-02 : 14 profiles\n",
      "2014-02 : complete.\n",
      "2014-03 : Begin indexing.\n",
      "2014-03 : 0 profiles\n",
      "2014-03 : complete.\n",
      "2014-04 : Begin indexing.\n",
      "2014-04 : 0 profiles\n",
      "2014-04 : complete.\n",
      "2014-05 : Begin indexing.\n",
      "2014-05 : 0 profiles\n",
      "2014-05 : complete.\n",
      "2014-06 : Begin indexing.\n",
      "2014-06 : 0 profiles\n",
      "2014-06 : complete.\n",
      "2014-07 : Begin indexing.\n",
      "2014-07 : 0 profiles\n",
      "2014-07 : complete.\n",
      "2014-08 : Begin indexing.\n",
      "2014-08 : 0 profiles\n",
      "2014-08 : complete.\n",
      "2014-09 : Begin indexing.\n",
      "2014-09 : 0 profiles\n",
      "2014-09 : complete.\n",
      "2014-10 : Begin indexing.\n",
      "2014-10 : 0 profiles\n",
      "2014-10 : complete.\n",
      "2014-11 : Begin indexing.\n",
      "2014-11 : 0 profiles\n",
      "2014-11 : complete.\n"
     ]
    }
   ],
   "source": [
    "def scrape(startyear, startmonth, endyear, endmonth):\n",
    "#   Walk the database through the defined ranges, downloading everything.\n",
    "    year = startyear\n",
    "    month = startmonth\n",
    "    while (not (year == endyear and month == endmonth)):\n",
    "        ys = \"{}\".format(year)\n",
    "        ms = \"{:02d}\".format(month)\n",
    "        gather_all_profiles(ys,ms) \n",
    "        if month == 12:\n",
    "            year += 1\n",
    "            month = 0\n",
    "        month += 1\n",
    "\n",
    "def gather_all_profiles(year, month):\n",
    "#   Walk the index pages, harvesting the profile URLs, and then download and process all the profiles stored under this year and month.\n",
    "    page = 1\n",
    "    urls = []\n",
    "\n",
    "    print(\"{}-{} : Begin indexing.\".format(year, month))\n",
    "\n",
    "    while (page > 0):\n",
    "        urlstring = \"http://scamdigger.com/{}/{}/page/{}\".format(year,month,page)    \n",
    "#         jitter = random.choice([0,1])\n",
    "#         print(urlstring)\n",
    "        try:\n",
    "            inhandle = urlopen(urlstring)\n",
    "#             print(inhandle)\n",
    "            urls.extend(enumerate_profiles(inhandle))\n",
    "#             print(urls)\n",
    "            page += 1\n",
    "        except:\n",
    "          page = 0\n",
    "\n",
    "    print(\"{}-{} : {} profiles\".format(year,month,len(urls)))\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            urlhandle = urlopen(url)\n",
    "            scrape_profile(urlhandle, year, month)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"Exception when handling {}\".format(url))\n",
    "            print(e)\n",
    "  \n",
    "    print(\"{}-{} : complete.\".format(year,month))\n",
    "    \n",
    "\n",
    "def enumerate_profiles(inhandle):\n",
    "    html = inhandle.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content = soup.findAll('div',{'class':'grid-thumb'})\n",
    "    urllist =[]\n",
    "    for link in content:\n",
    "        urllist.append(link.find('a')['href'])\n",
    "  \n",
    "    return urllist    \n",
    "\n",
    "def scrape_profile(inhandle,year,month):\n",
    "#     Scrape an input scamdiggers page for the profile content of the scammer.\n",
    "\n",
    "#Read file    \n",
    "    html = inhandle.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#Find main page content\n",
    "    content = soup.find('div', {'class':'entry-content'})\n",
    "    profile = {}\n",
    "\n",
    "#Fill in known info from URL\n",
    "    profile['year_reported'] = year\n",
    "    profile['month_reported'] = month\n",
    "\n",
    "    #Get visible text\n",
    "    text = content.get_text()\n",
    "#     print(text)\n",
    "\n",
    "    #Parse information from text\n",
    "    for key in extractors:\n",
    "        match = extractors[key].search(text)\n",
    "        if match:\n",
    "            matchtext = match.group(1)\n",
    "            if key in ['justifications','messages']:\n",
    "                vals = matchtext.split('\\n')\n",
    "            else:\n",
    "                vals = matchtext\n",
    "                profile[key] = vals \n",
    "\n",
    "    #Parse annotations\n",
    "    content1 = soup.find('div', {'class':'entry-utility'})\n",
    "    profile['tags']   = [node.get_text() for node in content1.findAll('a', {'rel':'tag'})]\n",
    "    profile['gender'] = 'female' if 'Female profiles' in profile['tags'] else 'male'\n",
    "#     print(profile)\n",
    "    \n",
    "    uid = profile['username'].strip()\n",
    "    outfile='scam'+uid+'.json'\n",
    "    \n",
    "    #Save output\n",
    "    profile['images'] = save_img(content, uid)\n",
    "    json.dump(profile, open(outfile,'w'))\n",
    "            \n",
    "def save_img(content, username): \n",
    "    \n",
    "    i = 1\n",
    "    image = []\n",
    "    for img in content.findAll('img'):\n",
    "        r = requests.get(img['src'], stream=True) #Get request on full_url\n",
    "        if r.status_code == 200:                     #200 status code = OK\n",
    "            outfile = username+str(i)+'.jpg'\n",
    "            try:\n",
    "                with open(outfile, 'wb') as f: \n",
    "                    r.raw.decode_content = True\n",
    "                    shutil.copyfileobj(r.raw, f)\n",
    "                    i+=1\n",
    "                    image.append(outfile)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(\"Exception when handling {}\".format(url))\n",
    "                print(e)\n",
    "    \n",
    "    return image\n",
    "\n",
    "    \n",
    "scrape(2014,1,2014,12) #Just change the date here. It will scrape everything. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2b7c93223ede03df5c98ec791ad28ce16e5ab6c1fe67d4b2c7f4a96f3159061"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
