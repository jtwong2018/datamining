{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b7b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = r\"C:\\Users\\christine\\Desktop\\new\\re\\*\" # change here\n",
    "images = glob(images_path +'*.jpg')\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "\n",
    "incept_model = ResNet50(include_top=True)\n",
    "\n",
    "from keras.models import Model\n",
    "last = incept_model.layers[-2].output\n",
    "modele = Model(inputs = incept_model.input,outputs = last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_features = {}\n",
    "count = 0\n",
    "for i in images:\n",
    "    img = cv2.imread(i)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    \n",
    "    img = img.reshape(1,224,224,3)\n",
    "    pred = modele.predict(img).reshape(2048,)\n",
    "        \n",
    "    img_name = i.split('/')[-1][34:]\n",
    "#     print(img_name)\n",
    "#     break\n",
    "    \n",
    "    images_features[img_name] = pred\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count > 1499:\n",
    "        break\n",
    "        \n",
    "    elif count % 50 == 0:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0aa2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_path = r\"C:\\Users\\christine\\Desktop\\DM PROJECT\\caption-generator\\Flickr8k_text\\Flickr8k.token.txt\" #change here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = open(caption_path, 'rb').read().decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = {}\n",
    "for i in captions:\n",
    "\n",
    "    name = i.split('\\t')\n",
    "    if len(name) == 2:\n",
    "        img_name = name[0][:-2]\n",
    "        cap = name[1]\n",
    "        \n",
    "        if img_name in images_features:\n",
    "            if img_name not in captions_dict:\n",
    "                captions_dict[img_name] = [cap]\n",
    "\n",
    "            else:\n",
    "                captions_dict[img_name].append(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ca705",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(captions_dict) # make sure not empty, should be 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed(txt):\n",
    "    modified = txt.lower()\n",
    "    modified = 'startofseq ' + modified + ' endofseq'\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in captions_dict.items():\n",
    "    for vv in v:\n",
    "        captions_dict[k][v.index(vv)] = preprocessed(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = {}\n",
    "for k,vv in captions_dict.items():\n",
    "    for v in vv:\n",
    "        for word in v.split():\n",
    "            if word not in count_words:\n",
    "\n",
    "                count_words[word] = 0\n",
    "\n",
    "            else:\n",
    "                count_words[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41043fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = -1\n",
    "count = 1\n",
    "new_dict = {}\n",
    "for k,v in count_words.items():\n",
    "    if count_words[k] > THRESH:\n",
    "        new_dict[k] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict['<OUT>'] = len(new_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd440ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_backup = captions_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = captions_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, vv in captions_dict.items():\n",
    "    for v in vv:\n",
    "        encoded = []\n",
    "        for word in v.split():  \n",
    "            if word not in new_dict:\n",
    "                encoded.append(new_dict['<OUT>'])\n",
    "            else:\n",
    "                encoded.append(new_dict[word])\n",
    "\n",
    "\n",
    "        captions_dict[k][vv.index(v)] = encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d02855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict # check that its all numbers now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 0\n",
    "for k, vv in captions_dict.items():\n",
    "    for v in vv:\n",
    "        if len(v) > MAX_LEN:\n",
    "            MAX_LEN = len(v)\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size = 5000\n",
    "VOCAB_SIZE = len(new_dict)\n",
    "\n",
    "def generator(photo, caption):\n",
    "    n_samples = 0\n",
    "    \n",
    "    X = []\n",
    "    y_in = []\n",
    "    y_out = []\n",
    "    \n",
    "    for k, vv in caption.items():\n",
    "        for v in vv:\n",
    "            for i in range(1, len(v)):\n",
    "                X.append(photo[k])\n",
    "\n",
    "                in_seq= [v[:i]]\n",
    "                out_seq = v[i]\n",
    "\n",
    "                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n",
    "\n",
    "                y_in.append(in_seq)\n",
    "                y_out.append(out_seq)\n",
    "            \n",
    "    return X, y_in, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ccc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_in, y_out = generator(images_features, captions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450dec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(y_in), len(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y_in = np.array(y_in, dtype='float64')\n",
    "y_out = np.array(y_out, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y_in.shape, y_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9288607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "max_len = MAX_LEN\n",
    "vocab_size = len(new_dict)\n",
    "\n",
    "image_model = Sequential()\n",
    "\n",
    "image_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\n",
    "image_model.add(RepeatVector(max_len))\n",
    "\n",
    "image_model.summary()\n",
    "\n",
    "language_model = Sequential()\n",
    "\n",
    "language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\n",
    "language_model.add(LSTM(256, return_sequences=True))\n",
    "language_model.add(TimeDistributed(Dense(embedding_size)))\n",
    "\n",
    "language_model.summary()\n",
    "\n",
    "conca = Concatenate()([image_model.output, language_model.output])\n",
    "x = LSTM(128, return_sequences=True)(conca)\n",
    "x = LSTM(512, return_sequences=False)(x)\n",
    "x = Dense(vocab_size)(x)\n",
    "out = Activation('softmax')(x)\n",
    "model = Model(inputs=[image_model.input, language_model.input], outputs = out)\n",
    "\n",
    "# model.load_weights(\"../input/model_weights.h5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X, y_in], y_out, batch_size=512, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da477394",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_dict = {v:k for k, v in new_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c199d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('mine_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150073f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vocab.npy', new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fb149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(x):\n",
    "    \n",
    "    test_img_path = images[x]\n",
    "\n",
    "    test_img = cv2.imread(test_img_path)\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    test_img = cv2.resize(test_img, (299,299))\n",
    "\n",
    "    test_img = np.reshape(test_img, (1,299,299,3))\n",
    "    \n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031ed8e",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6141d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    no = np.random.randint(1500,7000,(1,1))[0,0]\n",
    "    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n",
    "    \n",
    "    test_img_path = images[no]\n",
    "    test_img = cv2.imread(test_img_path)\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "    text_inp = ['startofseq']\n",
    "\n",
    "    count = 0\n",
    "    caption = ''\n",
    "    while count < 25:\n",
    "        count += 1\n",
    "\n",
    "        encoded = []\n",
    "        for i in text_inp:\n",
    "            encoded.append(new_dict[i])\n",
    "\n",
    "        encoded = [encoded]\n",
    "\n",
    "        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "\n",
    "\n",
    "        prediction = np.argmax(model.predict([test_feature, encoded]))\n",
    "\n",
    "        sampled_word = inv_dict[prediction]\n",
    "\n",
    "        caption = caption + ' ' + sampled_word\n",
    "            \n",
    "        if sampled_word == 'endofseq':\n",
    "            break\n",
    "\n",
    "        text_inp.append(sampled_word)\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.imshow(test_img)\n",
    "    plt.xlabel(caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
